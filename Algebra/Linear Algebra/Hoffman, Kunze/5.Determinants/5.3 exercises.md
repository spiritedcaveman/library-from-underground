

Vandermonde Matrix

$\begin{pmatrix} 1&1&\cdots & 1 &1\\a_1&a_2&\cdots &a_n&a_{n+1}\\\vdots&\vdots&\ddots&\vdots&\vdots\\a_1^{n-1}&a_2^{n-1}&\cdots&a_n^{n-1}& a_{n+1}^{n-1}\\a_1^{n}&a_2^{n}&\cdots&a_n^{n}&a_{n+1}^{n}\end{pmatrix}$

By subtracting $a_1$ times the $i$-th row to the $i+1$-th row, you get

$\begin{pmatrix} 1&1&\cdots & 1 &1\\   0&a_2-a_1&\cdots &a_n-a_1&a_{n+1}-a_1\\   \vdots&\vdots&\ddots&\vdots&\vdots\\   0&a_2^{n}-a_1a_{2}^{n-1}&\cdots&a_n^n-a_1a_{n}^{n-1}& a_{n+1}^{n}-a_1a_{n+1}^{n-1}\end{pmatrix}$

Expanding by the first column and factoring $a_i-a_1$ from the $i$-th column for $i=2,\ldots,n+1$, you get the determinant is 

$$=\prod_{j=2}^{n+1}(a_j-a_1) \det\begin{pmatrix} 1& 1&\cdots&1\\a_2&a_3&\cdots&a_{n+1}\\ \vdots&\vdots&\ddots&\vdots\\a_2^{n-1}&a_3^{n-1}&\cdots&a_{n+1}^{n-1}\end{pmatrix}$$

You may apply your inductive hypothesis, to get this is $$=\prod_{j=2}^{n+1}(a_j-a_1) \prod_{2\leqslant i<j\leqslant n+1}(a_j-a_i)=\prod_{1\leqslant i<j\leqslant n+1}(a_j-a_i)$$ and the inductive step is complete.


# 3.
the alternating group $A_3 = { \text{id}, (123), (132) }$ contains the even permutations. Let the $3 \times 3$ matrix be: $$\begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix}$$
Then the determinant formula (permutation expansion) is:  $\det(A) = {} + a_{11}a_{22}a_{33} \quad \text{(id)} \ + a_{12}a_{23}a_{31} \quad \text{(123)} \\+ a_{13}a_{21}a_{32} \quad \text{(132)} \ - a_{13}a_{22}a_{31} \quad \text{(12)} \ - a_{11}a_{23}a_{32} \quad \text{(13)} \ - a_{12}a_{21}a_{33} \quad \text{(23)}$
compactly: $$\det(A) = \sum_{\sigma \in S_3} \operatorname{sgn}(\sigma) \cdot a_{1,\sigma(1)} a_{2,\sigma(2)} a_{3,\sigma(3)}$$
   
# 4.

$$\sigma = \begin{pmatrix}1&2&3&4\\2&3&4&1\end{pmatrix} = (1\ 2\ 3\ 4) = (1\ 4)(1\ 3)(1\ 2) \text{ — 3 transpositions } \to \text{odd}$$
$$\tau = \begin{pmatrix}1&2&3&4\\3&1&2&4\end{pmatrix} = (1\ 3\ 2) = (1\ 2)(1\ 3) \text{ — 2 transpositions } \to \text{even}$$


$$\sigma \tau = \begin{pmatrix}1&2&3&4\\4&2&3&1\end{pmatrix} = (1\ 4)(2)(3) = (1\ 4) \text{ — 1 transposition } \to \text{odd}$$
$$\tau \sigma = \begin{pmatrix}1&2&3&4\\1&2&4&3\end{pmatrix} = (3\ 4) \text{ — 1 transposition } \to \text{odd}$$



# 5.

go fuck yourself
# 6.

Let $$A=\begin{pmatrix}
a &b\\
c & d
\end{pmatrix}
$$
then 
$$\det(A+1)=\det \begin{pmatrix}
a+1 &b\\
c & d+1
\end{pmatrix}=(a+1)(d+1)-bc
$$
on the other hand
$$\det(A)+1=\det \begin{pmatrix}
a &b\\
c & d
\end{pmatrix}+1=ad-bc+1
$$

Since $\det(A+I)=\det A+1$ you can conclude that $a+d=0$, hence $tr A=0$.


Assume $\operatorname{tr}(A) = 0$, so $a + d = 0$.
Then:
$$\det(I + A) = \det\begin{pmatrix}1 + a & b \\ c & 1 + d\end{pmatrix} = (1 + a)(1 + d) - bc$$
Since $a + d = 0$, we have $1 + d = 1 - a$, so:
$$\det(I + A) = (1 + a)(1 - a) - bc = 1 - a^2 - bc$$
Also:
$$\det A = ad - bc = -a^2 - bc \quad (\text{since } d = -a)$$
Then:
$$\det(I + A) = 1 + (-a^2 - bc) = 1 + \det A$$



# 7.

Let $$A = \begin{pmatrix} a_{11} & a_{12} & \cdots  & a_{1n}\\ & a_{22} & \cdots & a_{2n}  \\ & & \ddots & \\ & & & a_{nn}\end{pmatrix}$$

be your upper triangular matrix.  Expanding the left most column, the cofactor expansion formula tells you that the determinant of $A$ is 

$$a_{11} \cdot \textrm{det} \begin{pmatrix} a_{22} & a_{22} & \cdots  & a_{2n}\\ & a_{33} & \cdots & a_{3n}  \\ & & \ddots & \\ & & & a_{nn}\end{pmatrix}$$
Now this smaller $(n-1)$ by $(n-1)$ matrix is also upper triangular, so you can compute it as $a_{22}$ times an $(n-2)$ by $(n-2)$ upper triangular determinant:

$$\textrm{det } A = a_{11} a_{22} \cdot  \textrm{det} \begin{pmatrix} a_{33} & a_{34} & \cdots  & a_{3n}\\ & a_{44} & \cdots & a_{4n}  \\ & & \ddots & \\ & & & a_{nn}\end{pmatrix}$$

Iterating this argument, you're eventually going to get 

$$\textrm{Det } A = a_{11} \cdots a_{n-2,n-2} \cdot \textrm{det} \begin{pmatrix} a_{n-1,n-1} & a_{n-1,n} \\ & a_{nn} \end{pmatrix} = a_{11} \cdots a_{nn}$$


turn a lower triangular matrix into an upper tirangular one by transposing it and the determinant stays invariant

proof of the fact

Use the [Leibniz definition of determinants](https://en.wikipedia.org/wiki/Leibniz_formula_for_determinants), that $$\det(A) = \sum\limits_{\sigma\in S_n}\text{sgn}(\sigma)\prod\limits_{i=1}^na_{\sigma(i),i}$$

Recognize that the transpose of a permutation matrix is simply the inverse of the permutation matrix and so has the same sign.

Your sum then consists of exactly the same terms before and after the transposition with the same signs as originally had, just perhaps appearing in a slightly different order.

_$$\det(A) = \sum_{\tau \in S_n} \operatorname{sgn}(\tau) \prod_{i=1}^n a_{i \tau(i)} = \sum_{\sigma \in S_n} \operatorname{sgn}(\sigma) \prod_{i=1}^n a_{\sigma(i) i}$$


# 8.

Let $A$ be a $3 \times 3$ matrix over $\mathbb{C}$. Define the characteristic polynomial:
$$f(x) = \det(xI - A)$$
Since $xI - A$ is a $3 \times 3$ matrix with degree-1 polynomials in $x$ on the diagonal, the determinant expands to a degree 3 polynomial in $x$, and the leading term comes from the product of the diagonal entries $x \cdot x \cdot x = x^3$. So $f(x)$ is monic of degree 3.
Let $f(x) = (x - c_1)(x - c_2)(x - c_3)$, the roots $c_i$ being the (complex) eigenvalues of $A$.
From Vieta's formulas:

The sum of the roots: $c_1 + c_2 + c_3 = \text{coefficient of } x^2 = -\operatorname{tr}(A)$ (since the $x^2$ coefficient in $f(x)$ is $- \operatorname{tr}(A)$), so
$$c_1 + c_2 + c_3 = \operatorname{tr}(A)$$
The product of the roots: $c_1 c_2 c_3 = (-1)^3 \det(-A) = (-1)^3 (-1)^3 \det(A) = \det(A)$.
Done.

# 9.
Let $u \in S_n$ be a permutation. Define the linear operator
$$T(x_1, \dots, x_n) = (x_{u(1)}, \dots, x_{u(n)})$$
on the vector space $F^n$.

Linearity: For $v, w \in F^n$ and $\lambda \in F$, we have
$$T(v + \lambda w)_i = (v_i + \lambda w_i)_{u(i)} = v_{u(i)} + \lambda w_{u(i)} = T(v)_i + \lambda T(w)_i$$
so $T$ is linear.

Invertibility: The inverse is $T^{-1}(x_1, \dots, x_n) = (x_{u^{-1}(1)}, \dots, x_{u^{-1}(n)})$, since $u^{-1}$ is also a permutation and applying $u$ then $u^{-1}$ restores the original vector.

Matrix representation: The matrix of $T$ with respect to the standard basis is a permutation matrix — a matrix obtained by permuting the identity matrix’s rows (or columns). Every transposition corresponds to a permutation matrix with determinant $-1$, and permutations are products of transpositions. So:
$$\det(T) = \pm 1$$
Hence $\det(T) \ne 0 \Rightarrow T$ is invertible.

# 10.

Let $V$ be the vector space of all functions $S \to F$, and let $W \subseteq V$ be the set of alternating $n$-linear functions on $S$.

 **$W$ is a subspace**
The zero function is alternating and $n$-linear.
The sum of two alternating $n$-linear functions is alternating and $n$-linear.
Scalar multiples preserve these properties.
So $W$ is a subspace.

 **Dimension**
It is a standard result (from multilinear algebra) that any alternating $n$-linear function $D$ on $n \times n$ matrices over a field $F$ satisfies:
$$D(A) = D(I) \cdot \det A$$Thus, $D$ is determined entirely by its value on the identity matrix. That is,$$W = \{c \cdot \det : c \in F\}$$
So all elements of $W$ are scalar multiples of the determinant function.

# 11. 

Let $T$ be a linear operator on $F^n$. Define
$$D_T(\alpha_1, \dots, \alpha_n) = \det(T\alpha_1, \dots, T\alpha_n).$$

(a) Show that $D_T$ is an alternating $n$-linear function.
Soln.
 $D_T$ is alternating and $n$-linear because $T$ is linear, and the determinant is multilinear and alternating in its columns. So applying $T$ to each $\alpha_i$ preserves $n$-linearity and alternation.


(b) If $c = \det(Te_1, \dots, Te_n)$, show that for any vectors $\alpha_1, \dots, \alpha_n$ we have
$$\det(T\alpha_1, \dots, T\alpha_n) = c \det(\alpha_1, \dots, \alpha_n).$$
Soln.

 $D_T$ is an alternating $n$-linear function, and any such function is a scalar multiple of the determinant. Since $D_T(e_1, \dots, e_n) = \det(Te_1, \dots, Te_n) = c$, we get:
$$D_T(\alpha_1, \dots, \alpha_n) = c \cdot \det(\alpha_1, \dots, \alpha_n)$$
(c) If $\mathcal{B}$ is any ordered basis for $F^n$ and $A$ is the matrix of $T$ in the ordered basis $\mathcal{B}$, show that $\det A = c$.
Soln.
 Let $\mathcal{B} = (\beta_1, \dots, \beta_n)$ be an ordered basis. Then $A$ is defined by $T\beta_j = \sum_i A_{ij} \beta_i$, so the matrix with columns $[T\beta_1]_{\mathcal{B}}, \dots, [T\beta_n]_{\mathcal{B}}$ is exactly $A$. Hence,
$$c = \det([T\beta_1]_{\mathcal{B}}, \dots, [T\beta_n]_{\mathcal{B}}) = \det A.$$

(d) What do you think is a reasonable name for the scalar $c$?

Soln.
 A natural name for $c$ is the determinant of $T$, since it equals the determinant of any matrix representing $T$ in a basis.
 
# 12.

If $\sigma$ is a permutation of degree $n$ and $A$ is an $n \times n$ matrix over the field $F$ with row vectors $\alpha_1, \dots, \alpha_n$, let $\sigma(A)$ denote the $n \times n$ matrix with row vectors $\alpha_{\sigma(1)}, \dots, \alpha_{\sigma(n)}$.

(a) Prove that $\det(\sigma(AB)) = \det(\sigma(A)) \det(B)$, and in particular that $\det(\sigma(A)) = \det(\sigma(I)) \det(A)$.

We know $\det(AB)$ is multilinear and alternating in the rows of $A$, and permutation of rows multiplies the determinant by $\operatorname{sgn}(\sigma)$:
$$\det(\sigma(AB)) = \operatorname{sgn}(\sigma)\det(AB) \\ = \operatorname{sgn}(\sigma)\det(A)\det(B) \\ = \det(\sigma(A))\det(B)$$
because $\det(\sigma(A)) = \operatorname{sgn}(\sigma)\det(A)$.
So,
$$\det(\sigma(AB)) = \det(\sigma(A))\det(B)$$and in particular, taking $B = I$ gives:$$\det(\sigma(A)) = \det(\sigma(I))\det(A)$$
as claimed.

(b) If $T$ is the linear operator of Exercise 9, prove that the matrix of $T$ in the standard ordered basis is $\sigma(I)$.
Soln.
 The matrix of $T$ in the standard basis is $\sigma(I)$ because $T(e_i) = e_{u(i)}$, so the $i$-th row of the matrix is $e_{u(i)} =$ the $u(i)$-th standard basis vector — this is exactly the $i$-th row of $\sigma(I)$.

(c) Is $\sigma^{-1}(I)$ the inverse matrix of $\sigma(I)$?
Soln.
 Yes. Permuting rows via $\sigma$ corresponds to multiplying on the left by $\sigma(I)$, a permutation matrix. Its inverse is $\sigma^{-1}(I)$ since permuting rows by $\sigma$ then $\sigma^{-1}$ returns the identity matrix.

(d) Is it true that $\sigma(A)$ is similar to $A$?

Soln. Yes. $\sigma(A)$ is similar to $A$ because row permutation is conjugation by a permutation matrix:
$$\sigma(A) = \sigma(I) A,$$and column permutation: $A' = A \sigma^{-1}(I) \Rightarrow$
$$\sigma(A) = \sigma(I) A \sigma^{-1}(I)$$
So $\sigma(A)$ is conjugate to $A$, hence similar.
# 13.
Prove that the sign function on permutations is unique in the following sense. If $f$ is any function which assigns to each permutation of degree $n$ an integer, and if $f(\sigma \tau) = f(\sigma) f(\tau)$, then $f$ is identically $0$, or $f$ is identically $1$, or $f$ is the sign function.

Soln.
Let $f: S_n \to \mathbb{Z}$ satisfy $f(\sigma\tau) = f(\sigma)f(\tau)$ for all $\sigma, \tau \in S_n$. Then $f(e)^2 = f(e)$, so $f(e) = 0$ or $1$.

If $f(e) = 0$, then for any $\sigma \in S_n$, we have $f(\sigma) = f(\sigma e) = f(\sigma)f(e) = f(\sigma) \cdot 0 = 0$, so $f$ is identically zero.

If $f(e) = 1$, then for any transposition $t$, we have $t^2 = e$, hence $f(t)^2 = f(e) = 1$, so $f(t) = \pm 1$. Since every $\sigma \in S_n$ is a product of transpositions, and $f$ is multiplicative, $f(\sigma) = (\pm 1)^k$ where $k$ is the number of transpositions in a decomposition of $\sigma$. If $f(t) = 1$ for all transpositions $t$, then $f(\sigma) = 1$ for all $\sigma \in S_n$, so $f$ is identically one. If $f(t) = -1$ for all transpositions $t$, then $f(\sigma) = (-1)^k$, which is the sign function.

Therefore, the only such functions are the zero function, the constant one function, and the sign function.