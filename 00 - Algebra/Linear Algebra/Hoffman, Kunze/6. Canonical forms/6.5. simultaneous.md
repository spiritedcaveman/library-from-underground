
# 1.
Find an invertible real matrix $P$ such that $P^{-1}AP$ and $P^{-1}BP$ are both diagonal, where $A$ and $B$ are the real matrices
(a) $A = \begin{bmatrix} 1 & 2 \\ 0 & 2 \end{bmatrix}$, $B = \begin{bmatrix} 3 & -8 \\ 0 & -1 \end{bmatrix}$
(b) $A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$, $B = \begin{bmatrix} 1 & a \\ a & 1 \end{bmatrix}$

(a) $A$ is not diagonalizable over $\mathbb{R}$ (Jordan block for eigenvalue $1,2$), so no such $P$ exists.

 No invertible real matrix $P$ exists.

(b) $A$ has eigenvalues $0,2$, and is diagonalizable.
$B$ has eigenvalues $1+a, 1-a$, and is also diagonalizable.

$A$ and $B$ **commute** $\Leftrightarrow AB = BA \Rightarrow a=0$
So only when $a=0$ do $A$ and $B$ share a basis of eigenvectors.

 When $a = 0$, both are diagonalizable and commute $\Rightarrow$ common diagonalization possible.

Let $P = \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix}$. Then $P^{-1}AP = \begin{bmatrix} 0 & 0 \\ 0 & 2 \end{bmatrix}$ and $P^{-1}BP = I$.

So, solution only exists **when $a = 0$**.


---
# 2.
Let $\mathcal{F}$ be a commuting family of $3 \times 3$ complex matrices. How many linearly independent matrices can $\mathcal{F}$ contain? What about the $n \times n$ case?


All matrices in $\mathcal{F}$ commute $\Rightarrow$ simultaneously triangularizable over $\mathbb{C}$.

Then $\dim \mathcal{F} \leq n$ if all are diagonalizable and simultaneously diagonalizable.

But in general, maximal dimension of a commuting family of $n \times n$ matrices is $n$ if semisimple,

and $n^2$ only if all matrices are scalar multiples of identity (trivial case),

but the maximum dimension of a commuting subalgebra of $M_n(\mathbb{C})$ is $n$ (when algebra is commutative and contains a regular semisimple matrix).
So:

For $n = 3$: at most 3 linearly independent commuting matrices in general.
For general $n$: at most $n$ linearly independent commuting matrices (in a maximal commutative subalgebra).

---

SOLUTION: This turns out to be quite the hard question, so I'm not sure what Hoffman \& Kunze had in mind, there's a general theorem from 1905 by I. Schur which says the answer is $\left[\frac{n^2}{4}\right] + 1$. A simpler proof was published in 1998 by M. Mirzakhani in the American Mathematical Monthly. I have extracted the proof for the case $n = 3$.

First we show:

**Case $n = 2$:** The maximum size of a set of linearly independent commuting triangularizable $2 \times 2$ matrices is two 

Suppose that $\{A_1, A_2, A_3\}$ are three linearly independent commuting upper-triangular $2 \times 2$ matrices. Let $V$ be the space generated by $\{A_1, A_2, A_3\}$. So $\dim(V) = 3$.

Write $A_i = \begin{bmatrix} 0 & N_i \\ 0 & M_i \end{bmatrix}$ where $N_i$ is $1 \times 2$. Since $\dim(V) = 3$ it cannot be that all three $M_i$'s are zero. Assume WLOG that $M_1 \neq 0$. Then WLOG $M_1 = 1$ and $M_2 = c_2 M_1$, $M_3 = c_3 M_1$ for some constants $c_2, c_3$. Let $B_2 = A_2 - c_2 A_1$ and $B_3 = A_3 - c_3 A_1$. Then $\{B_2, B_3\}$ are linearly independent in $V$.

Write $B_2 = \begin{bmatrix} 0 & \tilde{N}_2 \\ 0 & 0 \end{bmatrix}$ and $B_3 = \begin{bmatrix} 0 & \tilde{N}_3 \\ 0 & 0 \end{bmatrix}$ where $\{\tilde{N}_2, \tilde{N}_3\}$ are linearly independent $1 \times 2$ matrices.

Similarly $\exists B_2', B_3'$ in $V$ such that $B_2' = \begin{bmatrix} 0 & 0 \\ \tilde{M}_2 & 0 \end{bmatrix}$, $B_3' = \begin{bmatrix} 0 & 0 \\ \tilde{M}_3 & 0 \end{bmatrix}$ where $\{\tilde{M}_2, \tilde{M}_3\}$ are linearly independent.

Since $B_2, B_3, B_2', B_3'$ are all in $V$, they all commute with each other. Thus $0 = [B_2', B_3] = 0 \forall i, j$.

Let $A$ be the $2 \times 2$ matrix $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$. Then $\text{rank}(A) = 2$ but $A \tilde{N}_2 = 0$ and $A \tilde{N}_3 = 0$ thus $\text{null}(A) = 2$. Therefore $\text{rank}(A) + \text{null}(A) = 4$. But $\text{rank}(A) + \text{null}(A)$ cannot be greater than $\dim(V) = 2$. This contradiction implies we cannot have $\{A_1, A_2, A_3\}$ all three be commuting linearly independent upper-triangular $2 \times 2$ matrices.

Because $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$ are such a pair.

We now turn to the case $n = 3$. Suppose $\mathcal{F}$ is a family of upper triangular commuting matrices of $\mathcal{F}$ = 4. We know $P^{-1} \mathcal{F} P$ is a commuting family of linearly independent $3 \times 3$ matrices generated by $\mathcal{F}$. Then $\dim(V) = 4$. Let $A_1, A_2, A_3, A_4$ be a linearly independent subset of $V$. For each $i$ $\exists$ a $2 \times 2$ matrix $M_i$ and a $1 \times 3$ matrix $N_i$ such that

$$
A_i = \begin{bmatrix} N_i \\ 0 & M_i \\ 0 & 0 \end{bmatrix}
$$

Since the $A_i$'s commute, for $1 \leq i, j \leq 4$ we have $M_i M_j = M_j M_i$. Suppose $W$ is the vector space spanned by the set $\{M_1, M_2, M_3, M_4\}$ and let $k = \dim(W)$. We know by (*) that $k \leq 2$. Since $\{A_1, A_2, A_3, A_4\}$ are independent we also know $k \geq 1$.

First assume $k = 1$. Then WLOG assume $M_1$ generates $W$. Then for $i = 2, 3, 4$ since each $M_i = n_i M_1$. For $i = 2, 3, 4$ define $B_i = A_i - n_i A_1$. So $\{A_1, A_2, A_3, A_4\}$ are linearly independent such that $M_i = n_i M_1$ and $B_i = \begin{bmatrix} \tilde{N}_i \\ 0 & 0 \\ 0 & 0 \end{bmatrix}$ where $\tilde{N}_i$ is $1 \times 3$ and $\{\tilde{N}_2, \tilde{N}_3, \tilde{N}_4\}$ are linearly independent.

Now assume $k = 2$. Then WLOG assume $M_1, M_2$ generate $W$. Then for each $i = 3, 4$ $\exists n_{i1}, n_{i2}$ such that $M_i = n_{i1} M_1 + n_{i2} M_2$. For $i = 3, 4$ define $B_i = A_i - n_{i1} A_1 - n_{i2} A_2$. Then $\{A_1, A_2, A_3, A_4\}$ linearly independent implies $\{B_3, B_4\}$ are linearly independent and $B_i = \begin{bmatrix} \tilde{N}_i \\ 0 & 0 \\ 0 & 0 \end{bmatrix}$ where $\tilde{N}_i$ is $1 \times 3$ and $\{\tilde{N}_3, \tilde{N}_4\}$ are linearly independent.

Thus in both cases ($k = 1, 2$) we have produced a set of $4 - k$ linearly independent $1 \times n$ matrices $\{\tilde{N}_i\}$ such that $B_i = \begin{bmatrix} \tilde{N}_i \\ 0 & 0 \\ 0 & 0 \end{bmatrix}$.

By a similar argument we obtain a set of two or three linearly independent $n \times 1$ matrices $\{\tilde{M}_i', \tilde{M}_j'\}$ or $\{\tilde{M}_i', \tilde{M}_j', \tilde{M}_k'\}$ such that $B_i' = \begin{bmatrix} 0 & \tilde{M}_i' \\ 0 & 0 \end{bmatrix}$ is a matrix in $V$.

Now since all the $B_i$'s and $B_i'$'s all belong to the commuting family $V$, one sees that $[B_i', B_j] = 0 \forall i, j$.

Let $A$ be the $m \times 4$ matrix ($m = 2$ or 3) such that its $i$th row is $t_i$. Since the $t_i$'s are independent we have $\text{rank}(A) \geq m \geq 2$. On the other hand $A \tilde{N}_i = 0$ for all $i$ the $t_i$'s linearly independent. Thus the null space of $A$ has rank or equal to the number of $\tilde{N}_i$'s. Thus $\text{rank}(A) + \text{null}(A) \geq 2$. But the $\text{rank}(A) + \text{null}(A) = 3$. This contradiction implies the set $\{A_1, A_2, A_3, A_4\}$ cannot be linearly independent.

Now we can achieve three independent such matrices because
\[
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}, \quad \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}, \quad \text{and} \quad \begin{bmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}
\]
are such a triple.


---

![[Pasted image 20250615101426.png]]


---

# 3.
Let $T$ be a linear operator on an $n$-dimensional space, and suppose that $T$ has $n$ distinct characteristic values. Prove that any linear operator which commutes with $T$ is a polynomial in $T$.


Since $T$ has $n$ distinct characteristic values over an $n$-dimensional space, it is diagonalizable with a basis of eigenvectors.

Let $v_1, \dots, v_n$ be eigenvectors of $T$, forming a basis, with $T(v_i) = \lambda_i v_i$, where the $\lambda_i$ are distinct.
Now suppose $U$ is a linear operator such that $UT = TU$.
We want to show that $U$ is a polynomial in $T$ â€” that is, $U = f(T)$ for some $f \in \mathbb{F}[x]$.


Since $T$ is diagonalizable, we can write:
$T = PDP^{-1},\quad \text{with } D = \operatorname{diag}(\lambda_1, \dots, \lambda_n).$
Because $UT = TU$, and $T$ is diagonalizable, $U$ preserves each eigenspace of $T$, and since each eigenspace is $1$-dimensional (distinct eigenvalues), $U$ acts by scalar multiplication on each eigenvector:
$U(v_i) = \mu_i v_i$
So, in the eigenbasis of $T$, $U$ is also diagonal:
$U = P\operatorname{diag}(\mu_1, \dots, \mu_n)P^{-1}$
Now apply the interpolation argument:
Define a function $f$ such that $f(\lambda_i) = \mu_i$ for all $i$. Since the $\lambda_i$ are distinct, the interpolation polynomial $f(x)$ of degree at most $n-1$ exists (via Lagrange interpolation).
Then $f(T)$ acts on $v_i$ as:
$f(T)(v_i) = f(\lambda_i) v_i = \mu_i v_i = U(v_i)$
Hence $f(T)$ and $U$ agree on a basis $\Rightarrow U = f(T)$.

---

Let $T$ be a linear operator on an $n$-dimensional vector space $V$ over a field $\mathbb{F}$, and suppose $T$ has $n$ distinct eigenvalues. Then $T$ is diagonalizable, and its minimal polynomial has degree $n$. Consider the set of all operators $U$ such that $UT = TU$; this set, called the commutant or centralizer of $T$, forms a subalgebra of $\operatorname{End}(V)$.

Since $T$ is diagonalizable with distinct eigenvalues, the commutant of $T$ is precisely the set of all polynomials in $T$, i.e., $\mathbb{F}[T]$. Indeed, any such $U$ must act diagonally in the basis of eigenvectors of $T$, and hence corresponds to evaluation of a polynomial at the eigenvalues. The space $\mathbb{F}[T]$ has dimension $n$ because the minimal polynomial of $T$ has degree $n$, making $\{I, T, T^2, \dots, T^{n-1}\}$ a basis. Thus, the space of all $U$ commuting with $T$ has dimension $n$, and consists exactly of the polynomials in $T$.

---
# 4.
Let $A, B, C$, and $D$ be $n \times n$ complex matrices which commute. Let $E$ be the $2n \times 2n$ matrix
$$E = \begin{bmatrix} A & B \\ C & D \end{bmatrix}.$$
Prove that $\det E = \det(AD - BC)$.

Since $A, B, C, D$ commute, they are simultaneously triangularizable. So there exists an invertible matrix $P$ such that $P^{-1}AP, P^{-1}BP, P^{-1}CP, P^{-1}DP$ are all upper triangular.
Define $Q = \begin{bmatrix} P & 0 \\ 0 & P \end{bmatrix}$. Then
$$Q^{-1}EQ = \begin{bmatrix} P^{-1}AP & P^{-1}BP \\ P^{-1}CP & P^{-1}DP \end{bmatrix}$$
has block upper triangular form with commuting upper triangular blocks. Thus we may assume WLOG that $A, B, C, D$ are upper triangular and commute.
In this setting, there is a standard formula: if $A, B, C, D$ are $n \times n$ complex matrices that commute, then
$\det\begin{bmatrix} A & B \\ C & D \end{bmatrix} = \det(AD - BC)$
This follows by expanding the determinant using block determinant identities valid under the commutativity assumption.
Hence, $\det E = \det(AD - BC)$.

---
# 5.
Let $\mathbb{F}$ be a field, $n$ a positive integer, and let $V$ be the space of $n \times n$ matrices over $\mathbb{F}$. If $A$ is a fixed $n \times n$ matrix over $\mathbb{F}$, let $T_A$ be the linear operator on $V$ defined by $T_A(B) = AB - BA$. Consider the family of linear operators $T_A$ obtained by letting $A$ vary over all diagonal matrices. Prove that the operators in that family are simultaneously diagonalizable.

Since $A, B, C, D$ commute and are complex $n \times n$ matrices, they are simultaneously diagonalizable. So there exists an invertible $P$ such that $P^{-1}AP = \operatorname{diag}(a_1, \dots, a_n)$, and similarly for $B, C, D$. Now consider the matrix $Q = \begin{bmatrix} P & 0 \\ 0 & P \end{bmatrix}$ acting on $\mathbb{C}^{2n}$. Then
$Q^{-1}EQ = \begin{bmatrix} P^{-1}AP & P^{-1}BP \\ P^{-1}CP & P^{-1}DP \end{bmatrix} = \begin{bmatrix} A' & B' \\ C' & D' \end{bmatrix}$
where $A', B', C', D'$ are diagonal. So $E$ is similar to a $2n \times 2n$ matrix with diagonals in all four blocks. Now permute the basis of $\mathbb{C}^{2n}$ so that the $i$-th row of the first $n$ rows is followed by the $i$-th row of the second $n$ rows, and similarly for columns â€” this gives a block diagonal matrix with $n$ many $2 \times 2$ blocks of the form
$\begin{bmatrix} a_i & b_i \\ c_i & d_i \end{bmatrix}.$
So now the determinant of $E$ is the product of the determinants of these $2 \times 2$ blocks:
$\det E = \prod_{i=1}^n (a_i d_i - b_i c_i),$
which equals $\det(AD - BC)$ since $A, B, C, D$ are diagonal.