

# 1.
Let $E$ be a projection of $V$ and let $T$ be a linear operator on $V$. Prove that the range of $E$ is invariant under $T$ if and only if $ETE = TE$. Prove that both the range and null space of $E$ are invariant under $T$ if and only if $ET = TE$.

If $ETE=TE$ then 
$$T(E(V))=E(T(E(V)))\leqslant E(V)$$
and so $E(V)$ is $T$-invariant. 

Conversely, if $E(V)$ is $T$-invariant then any $TE(x)$ is some $E(y)$, and then 
$$ETE(x)=EE(y)=E(y)=TE(x)\ \text{and so}\ ETE=TE.$$


For the rest, write $F=I-E$ and check in the usual way that $F$ is also a projection, and that moreover $\text{im} F=\ker E$ and $\text{im} E=\ker F$.

We then have (by the first part) that $\ker E=\text{im} F$ is $T$-invariant if and only if $FTF=TF$. Putting $F=1-E$ and simplifying we get that $\ker E$ is $T$-invariant if and only if $ETE=ET$.

Hence both image and kernel are $T$-invariant if and only if $TE=ET$. 

---
# 2.
Let $T$ be the linear operator on $\mathbb{R}^2$, the matrix of which in the standard ordered basis is
$$\begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}.$$
Let $W_1$ be the subspace of $\mathbb{R}^2$ spanned by the vector $\epsilon_1 = (1, 0)$.
(a) Prove that $W_1$ is invariant under $T$.

$$T(\epsilon_1) = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \end{bmatrix} = 2\epsilon_1 \in W_1.$$
Since $T(\epsilon_1) \in W_1$ and $W_1 = \text{span}\{\epsilon_1\}$, $W_1$ is invariant under $T$.


(b) Prove that there is no subspace $W_2$ which is invariant under $T$ and which is complementary to $W_1$:
$$\mathbb{R}^2 = W_1 \oplus W_2.$$

Suppose such a $W_2$ exists with $\mathbb{R}^2 = W_1 \oplus W_2$ and both $W_1$, $W_2$ $T$-invariant.
Then $T$ would be diagonalizable, since it would act separately on each 1D subspace.
But the matrix of $T$ is
$$\begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix},$$
which is not diagonalizable (it's a Jordan block with one eigenvector). Contradiction.
So no such $W_2$ exists.


(Compare with Exercise 1 of Section [[6.5. simultaneous]])

---

# 3.
Let $T$ be a linear operator on a finite-dimensional vector space $V$. Let $R$ be the range of $T$ and let $N$ be the null space of $T$. Prove that $R$ and $N$ are independent if and only if $V = R \oplus N$.

We say subspaces $R$ and $N$ are independent if $R \cap N = \{0\}$. If also $\dim R + \dim N = \dim V$, then by the dimension formula, we must have $V = R \oplus N$.
So:

If $R \cap N = \{0\}$ and $\dim R + \dim N = \dim V$, then $V = R \oplus N$.
Conversely, if $V = R \oplus N$, then $R \cap N = \{0\}$ (since it's a direct sum), and $\dim R + \dim N = \dim V$.
Hence, $R$ and $N$ are independent if and only if $V = R \oplus N$.

---
   
# 4.
Let $T$ be a linear operator on $V$. Suppose $V = W_1 \oplus \dots \oplus W_k$ where each $W_i$ is invariant under $T$. Let $T_i$ be the induced (restriction) operator on $W_i$.
(a) Prove that $\det(T) = \det(T_1) \dots \det(T_k)$.

Since $V = W_1 \oplus \dots \oplus W_k$ and each $W_i$ is invariant under $T$, we can choose a basis for $V$ by concatenating bases for each $W_i$. In this basis, the matrix of $T$ becomes block-diagonal:
$$[T] = \begin{bmatrix}
[T_1] & 0 & \cdots & 0 \\
0 & [T_2] & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & [T_k]
\end{bmatrix}$$The determinant of a block-diagonal matrix is the product of the determinants of the blocks. Hence,$$\det(T) = \det(T_1) \cdot \dots \cdot \det(T_k).$$

---

(b) Prove that the characteristic polynomial for $T$ is the product of the characteristic polynomials for $T_1, \dots, T_k$.

 In the same block-diagonal basis, the matrix of $T$ is block-diagonal with blocks $T_1, \dots, T_k$. The characteristic polynomial of a block-diagonal matrix is the product of the characteristic polynomials of the blocks. So:
$$\chi_T(t) = \chi_{T_1}(t) \cdots \chi_{T_k}(t).$$

---

(c) Prove that the minimal polynomial for $T$ is the least common multiple of the minimal polynomials for $T_1, \dots, T_k$. ($\textit{Hint}$: *Prove and then use the corresponding facts about direct sums of matrices.*)

 For minimal polynomials, a direct sum of operators has minimal polynomial equal to the least common multiple of the minimal polynomials of the summands:$$m_T(t) = \operatorname{lcm}(m_{T_1}(t), \dots, m_{T_k}(t)).$$
This holds because any polynomial that annihilates each $T_i$ also annihilates $T$, and the smallest such is the lcm.

---

# 5.
Let $T$ be the diagonalizable linear operator on $\mathbb{R}^3$ which we discussed in Example 3 of Section 6.2. Use the Lagrange polynomials to write the representing matrix $A$ in the form $A = c_1 E_1 + c_2 E_2 + E_3 = I$, $E_i E_j = 0$, $i \ne j$.

[hint](https://math.stackexchange.com/questions/1703853/how-to-use-lagrange-polynomials-to-express-the-matrix-of-a-linear-operator)

We are given that $T$ is diagonalizable with eigenvalues $1$ and $2$, with algebraic multiplicities $1$ and $2$, respectively. We use the Lagrange interpolation formula to write
$$A = 1 \cdot E_1 + 2 \cdot E_2.$$We already computed:$$E_1 = 2I - A = \begin{bmatrix} -3 & 6 & 6 \\ 1 & -2 & -2 \\ -3 & 6 & 6 \end{bmatrix}, \quad E_2 = A - I = \begin{bmatrix} 4 & -6 & -6 \\ -1 & 3 & 2 \\ 3 & -6 & -5 \end{bmatrix}.$$Now check that:$$E_1 + E_2 = (2I - A) + (A - I) = I,$$and:$$A = 1 \cdot E_1 + 2 \cdot E_2 = E_1 + 2E_2.$$Compute:$$E_1 + 2E_2 = \begin{bmatrix} -3 & 6 & 6 \\ 1 & -2 & -2 \\ -3 & 6 & 6 \end{bmatrix} + 2 \cdot \begin{bmatrix} 4 & -6 & -6 \\ -1 & 3 & 2 \\ 3 & -6 & -5 \end{bmatrix} = \begin{bmatrix} 5 & -6 & -6 \\ -1 & 4 & 2 \\ 3 & -6 & -4 \end{bmatrix} = A.$$Hence,$$A=E_1+2E_2, \quad I=E_1+E_2, \quad E_1E_2=0=E_2E_1.$$

---

# 6.
Let $A$ be the $4 \times 4$ matrix in Example 6 of Section 6.3. Find matrices $E_1, E_2, E_3$ such that $A = c_1 E_1 + c_2 E_2 + c_3 E_3$, $E_1 + E_2 + E_3 = I$, and $E_i E_j = 0, i \ne j$.



$$A=\begin{bmatrix} 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix}.$$The eigenvalues of $A$ (from that example) are:$$c_1 = 2, \quad c_2 = -2, \quad c_3 = 0,$$
with eigenspaces of dimensions 1, 1, and 2 respectively.

We define Lagrange polynomials:

$$
\begin{align} p_1(x) &= \frac{(x + 2)(x)}{(2 + 2)(2)} = \frac{(x + 2)x}{8}, \\ p_2(x) &= \frac{(x - 2)(x)}{(-2 - 2)(-2)} = \frac{(x - 2)x}{8}, \\ p_3(x) &= \frac{(x - 2)(x + 2)}{(0 - 2)(0 + 2)} = \frac{(x - 2)(x + 2)}{-4} = -\frac{x^2 - 4}{4}. \end{align}
$$

Now, define projection matrices as:
$$E_1 = p_1(A), \quad E_2 = p_2(A), \quad E_3 = p_3(A).$$We compute powers of $A$ (known from the example or computed directly):$$A^2 = \begin{bmatrix} 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \end{bmatrix}, \quad A^3 = 2A.$$
Now plug into polynomials:

Compute $E_1 = p_1(A) = \frac{1}{8}(A^2 + 2A)$
$$E_1 = \frac{1}{8}(A^2 + 2A) = \frac{1}{8} \left( \begin{bmatrix} 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \end{bmatrix} + 2 \begin{bmatrix} 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix} \right) = \frac{1}{8} \begin{bmatrix} 2 & 2 & 2 & 2 \\ 2 & 2 & 2 & 2 \\ 2 & 2 & 2 & 2 \\ 2 & 2 & 2 & 2 \end{bmatrix} \Rightarrow E_1 = \frac{1}{4} \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \end{bmatrix}$$
Compute $E_2 = p_2(A) = \frac{1}{8}(A^2 - 2A)$
Same method:
$$E_2 = \frac{1}{8}(A^2 - 2A) = \frac{1}{8} \left( \begin{bmatrix} 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \end{bmatrix} - 2 \begin{bmatrix} 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix} \right) = \frac{1}{8} \begin{bmatrix} 2 & -2 & 2 & -2 \\ -2 & 2 & -2 & 2 \\ 2 & -2 & 2 & -2 \\ -2 & 2 & -2 & 2 \end{bmatrix} \Rightarrow E_2 = \frac{1}{4} \begin{bmatrix} 1 & -1 & 1 & -1 \\ -1 & 1 & -1 & 1 \\ 1 & -1 & 1 & -1 \\ -1 & 1 & -1 & 1 \end{bmatrix}$$Compute $E_3 = p_3(A) = -\frac{1}{4}(A^2 - 4I)$$$E_3 = -\frac{1}{4}(A^2 - 4I) = \frac{1}{4}(4I - A^2) = \frac{1}{4} \left( 4 \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} - \begin{bmatrix} 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 0 & 2 \end{bmatrix} \right) = \frac{1}{4} \begin{bmatrix} 2 & 0 & -2 & 0 \\ 0 & 2 & 0 & -2 \\ -2 & 0 & 2 & 0 \\ 0 & -2 & 0 & 2 \end{bmatrix}$$Thus, the final answer is:$$E_1 = \frac{1}{4} \begin{bmatrix} 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \\ 1 & 1 & 1 & 1 \end{bmatrix}$$$$E_2 = \frac{1}{4} \begin{bmatrix} 1 & -1 & 1 & -1 \\ -1 & 1 & -1 & 1 \\ 1 & -1 & 1 & -1 \\ -1 & 1 & -1 & 1 \end{bmatrix}$$$$E_3 = \frac{1}{4} \begin{bmatrix} 2 & 0 & -2 & 0 \\ 0 & 2 & 0 & -2 \\ -2 & 0 & 2 & 0 \\ 0 & -2 & 0 & 2 \end{bmatrix}$$and$$A=2E_1-2E_2+0\cdot E_3=2E_1-2E_2, \quad E_1+E_2+E_3=I, \quad E_iE_j=0 \quad (i\neq j).$$

---

# 7.
In Exercises 5 and 6, notice that (for each $i$) the space of characteristic vectors associated with the characteristic value $c_i$ is spanned by the column vectors of the various matrices $E_i$ with $j \ne i$. Is that a coincidence?

No, it is not a coincidence — it follows directly from the construction of the projection matrices $E_i$ via the Lagrange interpolation polynomials.
Each $E_i = p_i(T)$ is a projection onto the $T$-invariant subspace corresponding to the eigenspace for $c_i$, and satisfies:
$$E_i^2 = E_i, \quad E_i E_j = 0 \text{ for } i \ne j, \quad \sum_i E_i = I.$$
So the image of $E_i$ is the eigenspace for $c_i$, and the kernel of $E_i$ is the direct sum of the eigenspaces for all $c_j$ with $j \ne i$.

Hence, the columns of $E_i$ lie in the eigenspace for $c_i$, and the columns of $E_j$ (with $j \ne i$) span the complement to that space — i.e., the sum of the other eigenspaces. So the span of the columns of $E_j$, $j \ne i$, equals the orthogonal complement (in the decomposition sense, not necessarily inner product) of the $c_i$-eigenspace.

Therefore, what you observed is an essential structural property of spectral decomposition, not a coincidence.

# 8.
Let $T$ be a linear operator on $V$ which commutes with every projection operator on $V$. What can you say about $T$?


Suppose $T$ commutes with every projection operator on $V$. Let $x \in V$ be arbitrary and nonzero. Let $E$ be the projection onto $\mathbb{C}x$ (or $\mathbb{R}x$ if $V$ is real). Then $E(x) = x$ and $\ker E = x^\perp$ (in some complement sense).

Now, since $TE = ET$, apply both sides to $x$:
$$TE(x) = T(x), \quad ET(x) = E(Tx).$$So $T(x) = E(Tx)$, meaning $T(x)$ lies in the image of $E$, i.e., in the span of $x$.
Thus, $T(x) = \lambda_x x$ for some scalar $\lambda_x$ — i.e., every vector is an eigenvector of $T$.
Now suppose $x, y \in V$ are linearly independent and $T(x) = \lambda_x x$, $T(y) = \lambda_y y$ with $\lambda_x \ne \lambda_y$. Then:
$$T(x+y)=\lambda_x x+\lambda_y y,$$
which is not a scalar multiple of $x + y$, contradicting the earlier result that all vectors are eigenvectors.
Hence, $\lambda_x = \lambda$ for all $x \ne 0$, and $T = \lambda I$.
Conclusion: If a linear operator $T$ commutes with all projections on $V$, then $T$ is a scalar multiple of the identity.

---


Let’s prove the converse:
If $T \in \mathrm{L}(V)$ commutes with every $S \in \mathrm{L}(V)$, then $T = \lambda I$ for some scalar $\lambda$.
Since $V$ is finite-dimensional, choose a basis and identify $T$ with a matrix $A \in M_n(F)$. The assumption that $T$ commutes with every linear operator becomes:
$$AS=SA \quad \text{for all } S \in M_n(F).$$So: $A$ commutes with all $n \times n$ matrices.
Now: in matrix theory, the only matrices that commute with all $n \times n$ matrices are the scalar multiples of the identity. That is,
$$\{A \in M_n(F) : AS = SA \text{ for all } S\} = \{ \lambda I : \lambda \in F \}.$$Hence, $A = \lambda I$, so $T = \lambda I$.

Conclusion:

$T$ commutes with every operator in $\mathrm{L}(V)$ $\Leftrightarrow$ $T = \lambda I$. 


---

# 9.
Let $V$ be the vector space of continuous real-valued functions on the interval $[-1, 1]$ of the real line. Let $W_e$ be the subspace of even functions, $f(-x) = f(x)$, and let $W_o$ be the subspace of odd functions, $f(-x) = -f(x)$.

[hint](https://math.stackexchange.com/questions/2817608/concerning-the-demonstration-of-w-e-perp-w-o-over-v-c-1-1)


**(a) Show that $V = W_e \oplus W_o$.**

 $V = W_e \oplus W_o$
For any $f \in V = C([-1,1], \mathbb{R})$, define:
$$f_e(x) = \frac{f(x) + f(-x)}{2}, \quad f_o(x) = \frac{f(x) - f(-x)}{2}.$$Then $f_e$ is even, $f_o$ is odd, and clearly $f = f_e + f_o$.
*Uniqueness: if $f = g_e + g_o$ with $g_e \in W_e$, $g_o \in W_o$, then $f_e - g_e = g_o - f_o$ is both even and odd $\Rightarrow$ must be $0$. So the decomposition is unique.*
Therefore, $V = W_e \oplus W_o$.


**(b) If $T$ is the indefinite integral operator**
**$$(Tf)(x) = \int_0^x f(t) \, dt$$**
**are $W_e$ and $W_o$ invariant under $T$?**



Let $f \in W_e$, i.e., $f(-t) = f(t)$. Then:
$$T(f)(-x) = \int_0^{-x} f(t)\,dt = -\int_{-x}^0 f(t)\,dt.$$Make substitution $t = -s$:
$$= -\int_{-x}^0 f(t)\,dt = -\int_x^0 f(-s)(-ds) = \int_0^x f(-s)\,ds = \int_0^x f(s)\,ds = T(f)(x).$$So $T(f)$ is even $\Rightarrow W_e$ is invariant under $T$.
Now let $f \in W_o$, so $f(-t) = -f(t)$. Then:
$$T(f)(-x) = \int_0^{-x} f(t)\,dt = -\int_{-x}^0 f(t)\,dt.$$Substitute $t = -s$ again:
$$= -\int_x^0 f(-s)(-ds) = \int_0^x (-f(s))\,ds = -T(f)(x).$$So $T(f)$ is even $\Rightarrow T(W_o) \subset W_e$.
Thus:

* $W_e$ is invariant under $T$,
* $W_o$ is not invariant — its image lies in $W_e$.
  
  