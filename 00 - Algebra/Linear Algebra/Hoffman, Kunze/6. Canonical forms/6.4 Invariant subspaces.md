

# 1.
Let $T$ be the linear operator on $\mathbb{R}^2$, the matrix of which in the standard ordered basis is
 $$A = \begin{bmatrix} 1 & -1 \\ 2 & -2 \end{bmatrix}.$$
(a) Prove that the only subspaces of $\mathbb{R}^2$ invariant under $T$ are $\mathbb{R}^2$ and the zero subspace.

Let $T$ be represented by $A = \begin{bmatrix} 1 & -1 \\ 2 & -2 \end{bmatrix}$ on $\mathbb{R}^2$. The characteristic polynomial is
$\chi_T(x) = \det(xI - A) = \begin{vmatrix} x-1 & 1 \\ -2 & x+2 \end{vmatrix} = (x - 1)(x + 2) + 2 = x^2 + x.$

So the eigenvalues are $0$ and $-1$, but we must check their geometric multiplicities. Compute:
$A - 0I = A \Rightarrow \operatorname{N}(A) = \{(x,y): x - y = 0, 2x - 2y = 0\} \Rightarrow \dim \ker A = 1,$
$A + I = \begin{bmatrix} 2 & -1 \\ 2 & -1 \end{bmatrix} \Rightarrow \operatorname{N}(A + I) = \{(x,y): 2x - y = 0\} \Rightarrow \dim \ker(A + I) = 1.$
So $T$ has two distinct eigenvalues, each with 1-dimensional eigenspaces. They span whole of $\Bbb{R}^{2}$


(b) If $U$ is the linear operator on $\mathbb{C}^2$, the matrix of which in the standard ordered basis is $A$, show that $U$ has 1-dimensional invariant subspaces.

Over $\mathbb{C}$, the same matrix $A$ defines $U$ on $\mathbb{C}^2$. The minimal polynomial $x(x+1)$ splits in $\mathbb{C}$, and the eigenspaces for $0$ and $-1$ are 1-dimensional as before. Hence, the corresponding eigenspaces
$E_0 = \operatorname{N}(A), \quad E_{-1} = \operatorname{N}(A + I)$
are complex 1-dimensional subspaces invariant under $U$. So $U$ has nontrivial invariant subspaces of dimension 1.

---

Let $A = \begin{bmatrix} 1 & -1 \\ 2 & 2 \end{bmatrix}$.


(a) The characteristic polynomial is

$\chi_T(x) = \det(xI - A) = \begin{vmatrix} x-1 & 1 \\ -2 & x-2 \end{vmatrix} = (x-1)(x-2) + 2 = x^2 - 3x + 4$.

This has no real roots (discriminant $< 0$), so $T$ has no real eigenvectors. Hence, no nontrivial $T$-invariant subspaces in $\mathbb{R}^2$ other than $\{0\}$ and $\mathbb{R}^2$.


(b) Over $\mathbb{C}$, the characteristic polynomial splits: $x^2 - 3x + 4 = (x - \tfrac{3}{2} + \tfrac{\sqrt{7}}{2}i)(x - \tfrac{3}{2} - \tfrac{\sqrt{7}}{2}i)$. So $U$ has two distinct complex eigenvalues and corresponding 1-dimensional eigenspaces, which are $U$-invariant.

---
# 2.
Let $W$ be an invariant subspace for $T$. Prove that the minimal polynomial for the restriction operator $T_W$ divides the minimal polynomial for $T$, without referring to matrices.

Let $T: V \to V$ be a linear operator on a finite-dimensional vector space $V$, and let $W \subseteq V$ be a $T$-invariant subspace. Define $T_W: W \to W$ by $T_W(w) = T(w)$ for $w \in W$. Let $m_T(x)$ and $m_{T_W}(x)$ be the minimal polynomials of $T$ and $T_W$, respectively.

Since $W$ is $T$-invariant, for all $w \in W$ and all $f(x) \in \mathbb{F}[x]$, we have $f(T)(w) = f(T_W)(w)$. In particular, $m_T(T)(w) = 0$ for all $w \in W$, so $m_T(T_W) = 0$ as an operator on $W$.

Hence, $m_T(x)$ is an annihilating polynomial for $T_W$. By definition, $m_{T_W}(x)$ is the monic polynomial of least degree such that $m_{T_W}(T_W) = 0$. Therefore, $m_{T_W}(x)$ divides $m_T(x)$ in $\mathbb{F}[x]$.

---
# 3.
Let $c$ be a characteristic value of $T$ and let $W$ be the space of characteristic vectors associated with the characteristic value $c$. What is the restriction operator $T_W$?

Let $T: V \to V$ be a linear operator and let $c \in \mathbb{F}$ be a characteristic value of $T$. Let
$W = \{ v \in V : T(v) = c v \}$ be the eigenspace associated to $c$.

Then $T_W: W \to W$ is defined by $T_W(v) = T(v) = c v$ for all $v \in W$. Thus, $T_W$ is the scalar multiplication operator by $c$ on $W$, i.e.,
$T_W = c \cdot \operatorname{id}_W.$

So the restriction of $T$ to its eigenspace $W$ is just multiplication by $c$.

---
# 4.
Let
$A = \begin{bmatrix} 0 & 1 & 0 \\ 2 & -2 & 2 \\ 2 & -3 & 2 \end{bmatrix}.$
Is $A$ similar over the field of real numbers to a triangular matrix? If so, find such a triangular matrix.


$\chi_A(x) = \det(xI - A) = \begin{vmatrix} x & -1 & 0 \\ -2 & x+2 & -2 \\ -2 & 3 & x-2 \end{vmatrix}.$
Use cofactor expansion along row 1:
$x \cdot \begin{vmatrix} x+2 & -2 \\ 3 & x-2 \end{vmatrix} + 1 \cdot \begin{vmatrix} -2 & -2 \\ -2 & x-2 \end{vmatrix}.$
Compute:
$x[(x+2)(x-2) + 6] + [(-2)(x-2) - (-2)(-2)] = x(x^2 - 4 + 6) + (-2x + 4 - 4) = x(x^2 + 2) - 2x = x^3 + 2x - 2x = x^3.$
So $\chi_A(x) = x(x^2 + 2)$. The roots are $0$, $i\sqrt{2}$, $-i\sqrt{2}$. Since these are not all real, $A$ is not similar over $\mathbb{R}$ to a real upper triangular matrix (triangularizable over $\mathbb{R}$ $\Leftrightarrow$ all eigenvalues real). Hence, $A$ is not similar over $\mathbb{R}$ to a triangular matrix.

---

# 5.
Every matrix $A$ such that $A^2 = A$ is similar to a diagonal matrix.

ONLY DIAGONALISABLE NILPOTENT MATRIX IS 0
---
# 6.
Let $T$ be a diagonalizable linear operator on the $n$-dimensional vector space $V$, and let $W$ be a subspace which is invariant under $T$. Prove that the restriction operator $T_W$ is diagonalizable.

Since $T$ is diagonalizable on $V$, there exists a basis $\mathcal{B} = \{v_1, \dots, v_n\}$ of $V$ consisting of eigenvectors of $T$. Let $W \subseteq V$ be $T$-invariant. Then for each $w \in W$, $T(w) \in W$, and $w$ can be expressed as a linear combination of the eigenvectors $v_i$.

Let $S = \{v_i \in \mathcal{B} : v_i \in W\}$. Since $T(v_i) = \lambda_i v_i$, the span of $S$ is preserved by $T$, and $T_W$ acts on $W$ via scalar multiplication on this basis. Thus, $S$ is a basis for $W$ (as a subspace of eigenvectors), and $T_W$ is diagonalizable.

---
# 7.
Let $T$ be a linear operator on a finite-dimensional vector space over the field of complex numbers. Prove that $T$ is diagonalizable if and only if $T$ is annihilated by some polynomial over $\mathbb{C}$ which has distinct roots.

Let $T: V \to V$ be a linear operator on a finite-dimensional vector space over $\mathbb{C}$.
($\Rightarrow$) If $T$ is diagonalizable, then there exists a basis of eigenvectors. Let the distinct eigenvalues be $\lambda_1, \dots, \lambda_k$. Then $(x - \lambda_1)\cdots(x - \lambda_k)$ annihilates $T$, since each eigenvector is sent to $0$ by this polynomial. So $T$ is annihilated by a polynomial with distinct roots.

($\Leftarrow$) Suppose $p(T) = 0$ for some polynomial $p(x)$ over $\mathbb{C}$ with distinct roots. Then $p(x)$ has no repeated irreducible factors, so it is square-free. By the Primary Decomposition Theorem, since $p$ splits completely and is square-free, $T$ is diagonalizable.
Thus, $T$ is diagonalizable if and only if it is annihilated by a polynomial with distinct roots over $\mathbb{C}$.


---
# 8.
Let $T$ be a linear operator on $V$. If every subspace of $V$ is invariant under $T$, then $T$ is a scalar multiple of the identity operator.

Suppose every subspace of $V$ is $T$-invariant. In particular, for each $v \in V$, the 1-dimensional subspace $\langle v \rangle$ is invariant, so $T(v) \in \langle v \rangle$, i.e., $T(v) = \lambda_v v$ for some scalar $\lambda_v$.
Now take any two linearly independent vectors $v, w \in V$. Since $\langle v + w \rangle$ is also invariant,
$T(v + w) = \lambda_{v+w}(v + w) = T(v) + T(w) = \lambda_v v + \lambda_w w.$

Equating gives $(\lambda_{v+w} - \lambda_v)v + (\lambda_{v+w} - \lambda_w)w = 0$.

Since $v, w$ are independent, this implies $\lambda_{v+w} = \lambda_v = \lambda_w$.
So $T(v) = \lambda v$ for all $v \in V$. Hence, $T = \lambda I$.

---
# 9.
Let $T$ be the indefinite integral operator

$$(Tf)(x) = \int_0^x f(t) \,dt$$

on the space of continuous functions on the interval $[0, 1]$. Is the space of polynomial functions invariant under $T$? The space of differentiable functions? The space of functions which vanish at $x = \frac{1}{2}$?

Let $T(f)(x) = \int_0^x f(t)\,dt$ on $C[0,1]$.

- Polynomials: If $f(x) = \sum a_n x^n$, then $Tf(x) = \sum \frac{a_n}{n+1} x^{n+1}$, which is again a polynomial. So the space of polynomial functions is invariant under $T$.

- Differentiable functions: If $f \in C^1[0,1]$, then $Tf \in C^2[0,1]$ by the Fundamental Theorem of Calculus. So $Tf$ is smoother than $f$, but still differentiable. Hence, the space of differentiable functions is invariant.

- Functions vanishing at $x = \tfrac{1}{2}$: Take $f$ such that $f(\tfrac{1}{2}) = 0$, but $Tf(\tfrac{1}{2}) = \int_0^{1/2} f(t)\,dt$ need not be zero. So this space is not invariant under $T$.

---
# 10.
Let $A$ be a $3 \times 3$ matrix with real entries. Prove that, if $A$ is not similar over $\mathbb{R}$ to a triangular matrix, then $A$ is similar over $\mathbb{C}$ to a diagonal matrix.

Since $A$ is a $3 \times 3$ real matrix, its characteristic polynomial is a real cubic. Because it is not similar over $\mathbb{R}$ to a triangular matrix[$T$ is triangulable over $F$ if and only if the minimal polynomial for $T$ is the product of linear polynomials over $F$.], it must not have three real eigenvaluesâ€”i.e., it does **not** split over $\mathbb{R}$. Therefore, it cannot have more than one real root (else it would factor into linear real terms), so it has exactly one real root.

By the conjugate root theorem, the remaining two complex roots must be non-real complex conjugates. Thus, the characteristic polynomial of $A$ splits over $\mathbb{C}$[$T$ is diagonalizable over $F$ if and only if the minimal polynomial for $T$ has distinct roots in $F$ with no repeats.] into three distinct linear factors (one real root, and a pair of non-real complex conjugates).

Hence, over $\mathbb{C}$, $A$ has three distinct eigenvalues and is therefore diagonalizable over $\mathbb{C}$.

---

# 11.
True or false? If the triangular matrix $A$ is similar to a diagonal matrix, then $A$ is already diagonal.

Since $A$ is similar to a diagonal matrix you know that, $$A = P^{-1}BP, $$ for some invertible matrix $P$ and diagonal matrix $B$. $A$ is called diagonalizable. Such a matrix is diagonalizable if it has all distinct eigenvalues. Such a matrix does not have to be diagonal to begin with.

say, $$
\begin{bmatrix}
1&1\\ 0&0
\end{bmatrix}
$$ is triangular and diagonalisable but not diagonal

> **If a triangular matrix A is similar to diagonal matrix, then A is already diagonal matrix**

**This holds true only when we add a constraint that all the diagonal elements of A are equal**

---
# 12.
Let $T$ be a linear operator on a finite-dimensional vector space over an algebraically closed field $\mathbb{F}$. Let $f$ be a polynomial over $\mathbb{F}$. Prove that $c$ is a characteristic value of $f(T)$ if and only if $c = f(t)$, where $t$ is a characteristic value of $T$.

Let $T$ be a linear operator on a finite-dimensional vector space $V$ over an algebraically closed field $\mathbb{F}$, and let $f \in \mathbb{F}[x]$. Suppose $\lambda$ is an eigenvalue of $T$, so there exists a nonzero $v \in V$ such that $Tv = \lambda v$. Applying $f(T)$ to $v$, we have
$f(T)v = f(\lambda)v,$
since $f(T)$ acts on $v$ as $f(\lambda)$ times the identity. Hence, $f(\lambda)$ is an eigenvalue of $f(T)$. This shows that if $\lambda$ is an eigenvalue of $T$, then $f(\lambda)$ is an eigenvalue of $f(T)$, i.e., every $f(t)$, with $t$ an eigenvalue of $T$, is an eigenvalue of $f(T)$.

Conversely, let $\mu$ be a characteristic value (eigenvalue) of $f(T)$, so there exists $v \ne 0$ with $f(T)v = \mu v$. Since $\mathbb{F}$ is algebraically closed, $T$ is triangularizable; in some basis, $T$ is upper triangular with eigenvalues $\lambda_1, \dots, \lambda_n$ on the diagonal. Then $f(T)$ is also upper triangular with diagonal entries $f(\lambda_1), \dots, f(\lambda_n)$. Thus, the eigenvalues of $f(T)$ are among the $f(\lambda_i)$, so $\mu=f(\lambda_i)$ for some $i$, i.e., $\mu=f(t)$ for some eigenvalue $t$ of $T$.
Therefore, $c$ is an eigenvalue of $f(T)$ if and only if $c=f(t)$ for some eigenvalue $t$ of $T$, as desired.

---

# 13.

Let $V$ be the space of $n \times n$ matrices over $\mathbb{F}$. Let $A$ be a fixed $n \times n$ matrix over $\mathbb{F}$. Let $T$ and $U$ be the linear operators on $V$ defined by
$T(B) = AB$
$U(B) = AB - BA.$

(a) True or false? If $A$ is diagonalizable (over $\mathbb{F}$), then $T$ is diagonalizable.

$T$ is the left-multiplication operator: $T(B) = AB$. Since $A$ is diagonalizable, there exists $P$ such that $A = PDP^{-1}$. Then $T$ is similar (as a linear operator on $M_n(\mathbb{F})$) to left-multiplication by $D$, which is diagonal. The operator $T$ will have eigenvalues equal to the eigenvalues of $A$, each repeated $n$ times, and it will be diagonalizable.
$\checkmark$ True.


(b) True or false? If $A$ is diagonalizable, then $U$ is diagonalizable.

$U$ is the commutator operator, and it acts like the adjoint action $\operatorname{ad}_A(B) = [A, B]$.
Even when $A$ is diagonalizable, the adjoint action need not be diagonalizable. For example, take $A = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}$. Then $U$ is not diagonalizable because its minimal polynomial can have repeated roots and insufficient eigenspaces.
$\times$ False.
